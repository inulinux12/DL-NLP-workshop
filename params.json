{"name":"Deep Learning and Natural Language Processing - workshop","tagline":"","body":"# Deep Learning and Natural Language Processing workshop.\r\nThis workshop will be held at Universidad Nacional de Colombia. Building 453(Aulas de ingeniería). We will show how deep learning meets natural language processing through 4 sessions. Each session will have a presentation and a corresponding jupyter notebook.\r\n\r\n## Schedule.\r\n* Tuesday 12th 9:00 - 12:00: Theano's introduction by Jorge Vanegas.\r\n  * Notebook and slides: [[Part 1](https://github.com/inulinux12/DL-NLP-workshop/blob/gh-pages/Introduction_To_Theano/IntroductionToTheano-Part1.ipynb)] and [[Part 2](https://github.com/inulinux12/DL-NLP-workshop/blob/gh-pages/Introduction_To_Theano/IntroductionToTheano-Part2.ipynb)]\r\n* Tuesday 12th 14:00 - 17:00: Getting started with Fuel and Blocks by John Arevalo.\r\n  * Presentation: [Fuel + Blocks](https://www.dropbox.com/s/pebmak2ikq0j81r/Fuel%20%2B%20Blocks%20introduction.pdf?dl=0) introduction.\r\n  * Notebooks: [Fuel server](https://github.com/inulinux12/DL-NLP-workshop/blob/gh-pages/Getting_Started_With_Fuel_And_Blocks/fuel_server.ipynb), [Fuel tutorial](https://github.com/inulinux12/DL-NLP-workshop/blob/gh-pages/Getting_Started_With_Fuel_And_Blocks/fuel_tutorial.ipynb) (This tutorial can be found also on [Slides and exercises](https://github.com/mila-udem/summerschool2015) for the Deep Learning Summer School 2015 programming tutorials), [Blocks basic example](https://github.com/inulinux12/DL-NLP-workshop/blob/gh-pages/Getting_Started_With_Fuel_And_Blocks/Blocks%20basic%20example.ipynb).\r\n* Wednesday 13th 9:00 - 12:00: Building a Neural Language Model with Blocks by John Arevalo.\r\n  * Notebook: [RNN tutorial](https://github.com/inulinux12/DL-NLP-workshop/blob/gh-pages/Building_A_Neural_Language_Model_With_Blocks/rnn-tutorial.ipynb) with blocks.\r\n* Wednesday 13th 14:00 - 17:00: Keras and Paraphrasing by Sebastian Sierra.\r\n  * Notebook: [Keras and NLP](https://github.com/inulinux12/DL-NLP-workshop/blob/gh-pages/Explore_Other_Tools_For_NLM/Keras_NLP.ipynb).\r\n  * Slides: Yet to come.\r\n* Thursday 14th 9:00 - 12:00: Tensorflow introduction by Sebastian Otálora.\r\n  * Slides:\r\n  * Notebook:\r\n* Thursday 14th 14:00 - 17:00: Practice: Build a text classification model.\r\n  * Slides:\r\n  * Notebook:\r\n\r\n## Useful links and resources.\r\n### Neural networks tools.    \r\nThis is a curated list of neural networks tools and frameworks. Most of them are written in Python and built on top of Theano. Most of them will be available from hal and lisi4 server.    \r\n* [Theano](http://deeplearning.net/software/theano/): A numerical computation library for Python.\r\n* [Blocks](https://github.com/mila-udem/blocks): Framework that helps you build neural network models on top of Theano.\r\n* [Torch](http://torch.ch/): Scientific computing framework with wide support for machine learning algorithms using Lua. \r\n* [Keras](http://keras.io/): A minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow or Theano.\r\n* [TensorFlow](https://www.tensorflow.org/): An open source software library for numerical computation using data flow graphs.\r\n* Gensim's [word2vec](https://radimrehurek.com/gensim/models/word2vec.html): An useful library built in Python that helps you build and train word2vec models.\r\n\r\n### Surveys.\r\nSurveys and empirical studies are useful resources describing how Recurrent Neural Networks behave. During 2015 there were several works related to this topic and these are some of the most important.    \r\n* Greff, K., Kumar Srivastava, R., Koutník, J., Steunebrink, B. R., & Schmidhuber, J. (2015). [LSTM: A Search Space Odyssey](http://arxiv.org/abs/1503.04069).\r\n* Karpathy, A., Johnson, J., & Fei-Fei, L. (2015). [Visualizing and Understanding Recurrent Networks](http://arxiv.org/abs/1506.02078).\r\n* Zachary C. Lipton. (2015). [A Critical Review of Recurrent Neural Networks for Sequence Learning](http://arxiv.org/abs/1506.00019v2).\r\n* Jozefowicz, R., Zaremba, W., & Sutskever, I. (2015). [An Empirical Exploration of Recurrent Network Architectures](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2015_jozefowicz15.pdf). In Proceedings of the 32nd International Conference on Machine Learning (Vol. 37).\r\n\r\n### Natural Language Processing.\r\n* [CS224d](http://cs224d.stanford.edu/index.html) Stanford Deep Learning for Natural Language Processing: Course materials [available](http://cs224d.stanford.edu/syllabus.html).\r\n* Advances of Deep Learning in NLP by Wojciech Zaremba, New York University and Facebook AI Research: [Presentation](http://www.cs.nyu.edu/~zaremba/docs/Advances%20in%20deep%20learning%20for%20NLP.pdf) available.\r\n\r\n### Recurrent neural networks.\r\nAlthough these resources could be in the NLP section, these are created as blogs or pages showing the power of Recurrent neural networks.    \r\n* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy.\r\n* [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Colah.\r\n* [Recurrent Neural Networks tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) by [wild-ml](http://www.wildml.com/) blog.\r\n* [Deep Learning Summer School](https://sites.google.com/site/deeplearningsummerschool/) 2015:\r\n  * [Video lectures](http://videolectures.net/deeplearning2015_montreal/)\r\n  * MILA's [Slides and exercises](https://github.com/mila-udem/summerschool2015) for the Deep Learning Summer School 2015 programming tutorials.\r\n\r\n### Applications \r\nThere are several applications of RNN for Natural Language Processing. Language Modelling, Text classification and Machine translation are some of the them.\r\n#### Language modelling\r\n* Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., & Fidler, S. (2015). [Skip-Thought Vectors](https://github.com/ryankiros/skip-thoughts). arXiv Preprint arXiv:1506.06726.\r\n* Li, J., Luong, M., & Jurafsky, D. (2015). [A Hierarchical Neural Autoencoder for Paragraphs and Documents](http://arxiv.org/pdf/1506.01057). In Acl 2015.\r\n* Tai, K. S., Socher, R., & Manning, C. D. (2015). [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://arxiv.org/abs/1503.00075). arXiv Preprint arXiv:1503.00075.\r\n\r\n## Further resources.\r\nThis list of links is based on Jiwon kim's [Awesome RNN](http://jiwonkim.org/awesome-rnn) list of resources.\r\n\r\n## About us\r\n![MindLab Research Group](http://i.imgur.com/CLrXU3M.png) [MindLab Research Group](https://sites.google.com/a/unal.edu.co/mindlab/)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}