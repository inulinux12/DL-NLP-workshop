{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TensorFlow: An Introduction *\n",
    "====================\n",
    "![MindLABLogo](images/mindlab-logo-simple.png \"MindLab\") \n",
    "![UNLogo](images/unal-logo.png \"MindLab\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To use TensorFlow we need to understand how TensorFlow:*\n",
    "* Represents computations as graphs (Theano alike)\n",
    "* Executes graphs in the context of Sessions\n",
    "* Represents data as tensors\n",
    "* Maintains state with Variables\n",
    "* Uses feeds and fetches to get data into and out of arbitrary operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing computations as graphs\n",
    "=========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simmilarly as happens with Theano, TensorFlow is built with the concept of variable dependence through a graph. \n",
    "* TensorFlow programs are usually structured into a construction phase, that assembles a graph, and an execution phase that uses a session to execute ops in the graph.\n",
    "* TensorFlow can be used from C, C++, and Python programs. It is presently much easier to use the Python library to assemble graphs, as it provides a large set of helper functions not available in the C and C++ libraries.\n",
    "* The session libraries have equivalent functionalities for the three languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=TensorShape([Dimension(1), Dimension(1)]), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
    "# added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "# Create another Constant that produces a 2x1 matrix.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
    "# The returned value, 'product', represents the result of the matrix\n",
    "# multiplication.\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "print(product)\n",
    "#print(product.get_shape())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default graph now has three nodes: two constant() ops and one matmul() op. To actually multiply the matrices, and get the result of the multiplication, we must launch the graph in a session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching the graph in a session\n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching follows construction. To launch a graph, create a Session object. Without arguments the session constructor launches the default graph.\n",
    "\n",
    "See the  [Session class](https://www.tensorflow.org/versions/master/api_docs/python/client.html#session-management \"Session\")  for the complete session API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the default graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# To run the matmul op we call the session 'run()' method, passing 'product'\n",
    "# which represents the output of the matmul op.  This indicates to the call\n",
    "# that we want to get the output of the matmul op back.\n",
    "#\n",
    "# All inputs needed by the op are run automatically by the session.  They\n",
    "# typically are run in parallel.\n",
    "#\n",
    "# The call 'run(product)' thus causes the execution of threes ops in the\n",
    "# graph: the two constants and matmul.\n",
    "#\n",
    "# The output of the op is returned in 'result' as a numpy `ndarray` object.\n",
    "result = sess.run(product)\n",
    "print(result)\n",
    "# ==> If everithing goes well we should see [[ 12.]]\n",
    "\n",
    "# Close the Session when we're done.\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of use in interactive Python environments, such as IPython you can instead use the InteractiveSession class, and the Tensor.eval() and Operation.run() methods. This avoids having to keep a variable holding the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2. -1.]\n"
     ]
    }
   ],
   "source": [
    "# Enter an interactive TensorFlow Session.\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.Variable([1.0, 2.0])\n",
    "a = tf.constant([3.0, 3.0])\n",
    "\n",
    "# Initialize 'x' using the run() method of its initializer op.\n",
    "x.initializer.run()\n",
    "\n",
    "# Add an op to subtract 'a' from 'x'.  Run it and print the result\n",
    "sub = tf.sub(x, a)\n",
    "print(sub.eval())\n",
    "# ==> [-2. -1.]\n",
    "\n",
    "# Close the Session when we're done.\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors and Variables\n",
    "===============================\n",
    "\n",
    "* *TensorFlow* programs use a <b>Tensor</b> data structure to represent all data -- only tensors are passed between operations in the computation graph. You can think of a TensorFlow tensor as an n-dimensional array or list. A tensor has a static type, a rank, and a shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Variables</b> maintain state across executions of the graph. The following example shows a variable serving as a simple counter. See Variables for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Create a Variable, that will be initialized to the scalar value 0.\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# Create an Op to add one to `state`.\n",
    "\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# Variables must be initialized by running an `init` Op after having\n",
    "# launched the graph.  We first have to add the `init` Op to the graph.\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph and run the ops.\n",
    "with tf.Session() as sess:\n",
    "  # Run the 'init' op\n",
    "  sess.run(init_op)\n",
    "  # Print the initial value of 'state'\n",
    "  print(sess.run(state))\n",
    "  # Run the op that updates 'state' and print 'state'.\n",
    "  for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print(sess.run(state))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The code of this notebook is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Usage example: Fitting a line \n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from: https://www.tensorflow.org/versions/master/get_started/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "x_data = np.random.rand(100).astype(\"float32\")\n",
    "y_data = x_data * 0.1 + 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 0.1 and b 0.3, but Tensorflow will\n",
    "# figure that out for us.)\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = W * x_data + b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Minimize the mean squared errors.\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([ 0.08645561], dtype=float32), array([ 0.4079203], dtype=float32))\n",
      "(20, array([ 0.08460048], dtype=float32), array([ 0.30784193], dtype=float32))\n",
      "(40, array([ 0.0956403], dtype=float32), array([ 0.30222011], dtype=float32))\n",
      "(60, array([ 0.09876575], dtype=float32), array([ 0.30062851], dtype=float32))\n",
      "(80, array([ 0.09965059], dtype=float32), array([ 0.30017793], dtype=float32))\n",
      "(100, array([ 0.0999011], dtype=float32), array([ 0.30005038], dtype=float32))\n",
      "(120, array([ 0.09997199], dtype=float32), array([ 0.30001429], dtype=float32))\n",
      "(140, array([ 0.09999207], dtype=float32), array([ 0.30000407], dtype=float32))\n",
      "(160, array([ 0.09999774], dtype=float32), array([ 0.30000117], dtype=float32))\n",
      "(180, array([ 0.09999936], dtype=float32), array([ 0.30000034], dtype=float32))\n",
      "(200, array([ 0.09999983], dtype=float32), array([ 0.3000001], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Before starting, initialize the variables.  We will 'run' this first.\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph.\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Fit the line.\n",
    "for step in xrange(201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(W), sess.run(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks in TensorFlow\n",
    "=============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is from the Penn tree bank project:\n",
    "https://www.cis.upenn.edu/~treebank/\n",
    "The data required for this tutorial is in the data/ directory of the PTB dataset from Tomas Mikolov's webpage: http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "\n",
    "The dataset is already preprocessed and contains overall 10000 different words, including the end-of-sentence marker and a special symbol (<unk>) for rare words. We convert all of them in the reader.py to unique integer identifiers to make it easy for the neural network to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/jsotaloram/nlp/tensorflow/tensorflow/models/rnn/ptb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Utilities for parsing PTB text files.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "\n",
    "def _read_words(filename):\n",
    "  with gfile.GFile(filename, \"r\") as f:\n",
    "    return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def _build_vocab(filename):\n",
    "  data = _read_words(filename)\n",
    "\n",
    "  counter = collections.Counter(data)\n",
    "  count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "\n",
    "  words, _ = list(zip(*count_pairs))\n",
    "  word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "  return word_to_id\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "  data = _read_words(filename)\n",
    "  return [word_to_id[word] for word in data]\n",
    "\n",
    "\n",
    "def ptb_raw_data(data_path=None):\n",
    "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "  Reads PTB text files, converts strings to integer ids,\n",
    "  and performs mini-batching of the inputs.\n",
    "  The PTB dataset comes from Tomas Mikolov's webpage:\n",
    "  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "  Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has\n",
    "      been extracted.\n",
    "  Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "  \"\"\"\n",
    "\n",
    "  train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "  valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "  test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "  word_to_id = _build_vocab(train_path)\n",
    "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "  vocabulary = len(word_to_id)\n",
    "  return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "  \"\"\"Iterate on the raw PTB data.\n",
    "  This generates batch_size pointers into the raw PTB data, and allows\n",
    "  minibatch iteration along these pointers.\n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "  Yields:\n",
    "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
    "    The second element of the tuple is the same data time-shifted to the\n",
    "    right by one.\n",
    "  Raises:\n",
    "    ValueError: if batch_size or num_steps are too high.\n",
    "  \"\"\"\n",
    "  raw_data = np.array(raw_data, dtype=np.int32)\n",
    "\n",
    "  data_len = len(raw_data)\n",
    "  batch_len = data_len // batch_size\n",
    "  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "  for i in range(batch_size):\n",
    "    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
    "\n",
    "  epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "  if epoch_size == 0:\n",
    "    raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "  for i in range(epoch_size):\n",
    "    x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "    yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example / benchmark for building a PTB LSTM model.\\nTrains the model described in:\\n(Zaremba, et. al.) Recurrent Neural Network Regularization\\nhttp://arxiv.org/abs/1409.2329\\nThere are 3 supported model configurations:\\n===========================================\\n| config | epochs | train | valid  | test\\n===========================================\\n| small  | 13     | 37.99 | 121.39 | 115.91\\n| medium | 39     | 48.45 |  86.16 |  82.07\\n| large  | 55     | 37.87 |  82.62 |  78.29\\nThe exact results may vary depending on the random initialization.\\nThe hyperparameters used in the model:\\n- init_scale - the initial scale of the weights\\n- learning_rate - the initial value of the learning rate\\n- max_grad_norm - the maximum permissible norm of the gradient\\n- num_layers - the number of LSTM layers\\n- num_steps - the number of unrolled steps of LSTM\\n- hidden_size - the number of LSTM units\\n- max_epoch - the number of epochs trained with the initial learning rate\\n- max_max_epoch - the total number of epochs for training\\n- keep_prob - the probability of keeping weights in the dropout layer\\n- lr_decay - the decay of the learning rate for each epoch after \"max_epoch\"\\n- batch_size - the batch size'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Example / benchmark for building a PTB LSTM model.\n",
    "Trains the model described in:\n",
    "(Zaremba, et. al.) Recurrent Neural Network Regularization\n",
    "http://arxiv.org/abs/1409.2329\n",
    "There are 3 supported model configurations:\n",
    "===========================================\n",
    "| config | epochs | train | valid  | test\n",
    "===========================================\n",
    "| small  | 13     | 37.99 | 121.39 | 115.91\n",
    "| medium | 39     | 48.45 |  86.16 |  82.07\n",
    "| large  | 55     | 37.87 |  82.62 |  78.29\n",
    "The exact results may vary depending on the random initialization.\n",
    "The hyperparameters used in the model:\n",
    "- init_scale - the initial scale of the weights\n",
    "- learning_rate - the initial value of the learning rate\n",
    "- max_grad_norm - the maximum permissible norm of the gradient\n",
    "- num_layers - the number of LSTM layers\n",
    "- num_steps - the number of unrolled steps of LSTM\n",
    "- hidden_size - the number of LSTM units\n",
    "- max_epoch - the number of epochs trained with the initial learning rate\n",
    "- max_max_epoch - the total number of epochs for training\n",
    "- keep_prob - the probability of keeping weights in the dropout layer\n",
    "- lr_decay - the decay of the learning rate for each epoch after \"max_epoch\"\n",
    "- batch_size - the batch size\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import seq2seq\n",
    "import reader\n",
    "\n",
    "flags = tf.flags\n",
    "logging = tf.logging\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"Model\", \"small\",\n",
    "    \"A type of Model. Possible options are: small, medium, large.\")\n",
    "flags.DEFINE_string(\"Data_path\", None, \"Data_path\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "class PTBModel(object):\n",
    "  \"\"\"The PTB model.\"\"\"\n",
    "\n",
    "  def __init__(self, is_training, config):\n",
    "    self.batch_size = batch_size = config.batch_size\n",
    "    self.num_steps = num_steps = config.num_steps\n",
    "    size = config.hidden_size\n",
    "    vocab_size = config.vocab_size\n",
    "\n",
    "    self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "    # Slightly better results can be obtained with forget gate biases\n",
    "    # initialized to 1 but the hyperparameters of the model would need to be\n",
    "    # different than reported in the paper.\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(size, forget_bias=0.0)\n",
    "    if is_training and config.keep_prob < 1:\n",
    "      lstm_cell = rnn_cell.DropoutWrapper(\n",
    "          lstm_cell, output_keep_prob=config.keep_prob)\n",
    "    cell = rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "\n",
    "    self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    with tf.device(\"/gpu:0\"): #setting the gpu 0 \n",
    "      embedding = tf.get_variable(\"embedding\", [vocab_size, size])\n",
    "      inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "    if is_training and config.keep_prob < 1:\n",
    "      inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "    # Simplified version of tensorflow.models.rnn.rnn.py's rnn().\n",
    "    # This builds an unrolled LSTM for tutorial purposes only.\n",
    "    # In general, use the rnn() or state_saving_rnn() from rnn.py.\n",
    "    #\n",
    "    # The alternative version of the code below is:\n",
    "    #\n",
    "    # from tensorflow.models.rnn import rnn\n",
    "    # inputs = [tf.squeeze(input_, [1])\n",
    "    #           for input_ in tf.split(1, num_steps, inputs)]\n",
    "    # outputs, states = rnn.rnn(cell, inputs, initial_state=self._initial_state)\n",
    "    outputs = []\n",
    "    states = []\n",
    "    state = self._initial_state\n",
    "    with tf.variable_scope(\"RNN\"):\n",
    "      for time_step in range(num_steps):\n",
    "        if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "        (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "        outputs.append(cell_output)\n",
    "        states.append(state)\n",
    "\n",
    "    output = tf.reshape(tf.concat(1, outputs), [-1, size])\n",
    "    logits = tf.nn.xw_plus_b(output,\n",
    "                             tf.get_variable(\"softmax_w\", [size, vocab_size]),\n",
    "                             tf.get_variable(\"softmax_b\", [vocab_size]))\n",
    "    loss = seq2seq.sequence_loss_by_example([logits],\n",
    "                                            [tf.reshape(self._targets, [-1])],\n",
    "                                            [tf.ones([batch_size * num_steps])],\n",
    "                                            vocab_size)\n",
    "    self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "    self._final_state = states[-1]\n",
    "\n",
    "    if not is_training:\n",
    "      return\n",
    "\n",
    "    self._lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                      config.max_grad_norm)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "    self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "  def assign_lr(self, session, lr_value):\n",
    "    session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "  @property\n",
    "  def input_data(self):\n",
    "    return self._input_data\n",
    "\n",
    "  @property\n",
    "  def targets(self):\n",
    "    return self._targets\n",
    "\n",
    "  @property\n",
    "  def initial_state(self):\n",
    "    return self._initial_state\n",
    "\n",
    "  @property\n",
    "  def cost(self):\n",
    "    return self._cost\n",
    "\n",
    "  @property\n",
    "  def final_state(self):\n",
    "    return self._final_state\n",
    "\n",
    "  @property\n",
    "  def lr(self):\n",
    "    return self._lr\n",
    "\n",
    "  @property\n",
    "  def train_op(self):\n",
    "    return self._train_op\n",
    "\n",
    "\n",
    "class SmallConfig(object):\n",
    "  \"\"\"Small config.\"\"\"\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 20\n",
    "  hidden_size = 200\n",
    "  max_epoch = 4\n",
    "  max_max_epoch = 13\n",
    "  keep_prob = 1.0\n",
    "  lr_decay = 0.5\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "\n",
    "class MediumConfig(object):\n",
    "  \"\"\"Medium config.\"\"\"\n",
    "  init_scale = 0.05\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 35\n",
    "  hidden_size = 650\n",
    "  max_epoch = 6\n",
    "  max_max_epoch = 39\n",
    "  keep_prob = 0.5\n",
    "  lr_decay = 0.8\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "\n",
    "class LargeConfig(object):\n",
    "  \"\"\"Large config.\"\"\"\n",
    "  init_scale = 0.04\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 10\n",
    "  num_layers = 2\n",
    "  num_steps = 35\n",
    "  hidden_size = 1500\n",
    "  max_epoch = 14\n",
    "  max_max_epoch = 55\n",
    "  keep_prob = 0.35\n",
    "  lr_decay = 1 / 1.15\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "\n",
    "def run_epoch(session, m, data, eval_op, verbose=False):\n",
    "  \"\"\"Runs the model on the given data.\"\"\"\n",
    "  epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "  start_time = time.time()\n",
    "  costs = 0.0\n",
    "  iters = 0\n",
    "  state = m.initial_state.eval()\n",
    "  for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size,\n",
    "                                                    m.num_steps)):\n",
    "    cost, state, _ = session.run([m.cost, m.final_state, eval_op],\n",
    "                                 {m.input_data: x,\n",
    "                                  m.targets: y,\n",
    "                                  m.initial_state: state})\n",
    "    costs += cost\n",
    "    iters += m.num_steps\n",
    "\n",
    "    if verbose and step % (epoch_size // 10) == 10:\n",
    "      print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "            (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "             iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "  return np.exp(costs / iters)\n",
    "\n",
    "\n",
    "def get_config():\n",
    "  if FLAGS.model == \"small\":\n",
    "    return SmallConfig()\n",
    "  elif FLAGS.model == \"medium\":\n",
    "    return MediumConfig()\n",
    "  elif FLAGS.model == \"large\":\n",
    "    return LargeConfig()\n",
    "  else:\n",
    "    raise ValueError(\"Invalid model: %s\", FLAGS.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reading the data from the extracted simple-examples.tgz\n",
    "data_path = '/home/jsotaloram/nlp/simple-examples/data/'\n",
    "raw_data = reader.ptb_raw_data(data_path)\n",
    "train_data, valid_data, test_data, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--Model MODEL] [--Data_path DATA_PATH]\n",
      "__main__.py: error: unrecognized arguments: -f /home/jsotaloram/.ipython/profile_default/security/kernel-1783fdc4-3ea1-43ad-adee-0abf8d6d4ad5.json --profile-dir /home/jsotaloram/.ipython/profile_default\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "#setting config vars\n",
    "FLAGS.model = \"small\"\n",
    "config = SmallConfig()\n",
    "eval_config = SmallConfig()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Android Example\n",
    "=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
