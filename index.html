<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Deep Learning and Natural Language Processing - workshop : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Deep Learning and Natural Language Processing - workshop</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/inulinux12/DL-NLP-workshop">View on GitHub</a>

          <h1 id="project_title">Deep Learning and Natural Language Processing - workshop</h1>
          <h2 id="project_tagline"></h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/inulinux12/DL-NLP-workshop/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/inulinux12/DL-NLP-workshop/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
<a id="deep-learning-and-natural-language-processing-workshop" class="anchor" href="#deep-learning-and-natural-language-processing-workshop" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep Learning and Natural Language Processing workshop.</h1>

<p>This workshop will be held at Universidad Nacional de Colombia. Building 453(Aulas de ingeniería). We will show how deep learning meets natural language processing through 4 sessions. Each session will have a presentation and a corresponding jupyter notebook.</p>

<h2>
<a id="schedule" class="anchor" href="#schedule" aria-hidden="true"><span class="octicon octicon-link"></span></a>Schedule.</h2>

<ul>
<li>Tuesday 12th 9:00 - 12:00: Theano's introduction by Jorge Vanegas.

<ul>
<li>Notebook and slides: <a href="https://www.dropbox.com/sh/f8080n3ovlkzsct/AAAqBFPcZM4sbxxgICibKmdDa?dl=0">Introduction to Theano</a>.</li>
</ul>
</li>
<li>Tuesday 12th 14:00 - 17:00: Getting started with Fuel and Blocks by John Arevalo.

<ul>
<li>Presentation: <a href="https://www.dropbox.com/s/pebmak2ikq0j81r/Fuel%20%2B%20Blocks%20introduction.pdf?dl=0">Fuel + Blocks</a> introduction.</li>
<li>Notebooks: <a href="https://github.com/inulinux12/DL-NLP-workshop/blob/gh-pages/Getting_Started_With_Fuel_And_Blocks/fuel_server.ipynb">Fuel server</a>, <a href="https://github.com/inulinux12/DL-NLP-workshop/blob/gh-pages/Getting_Started_With_Fuel_And_Blocks/fuel_tutorial.ipynb">Fuel tutorial</a>, <a href="https://github.com/inulinux12/DL-NLP-workshop/blob/gh-pages/Getting_Started_With_Fuel_And_Blocks/Blocks%20basic%20example.ipynb">Blocks basic example</a>.</li>
</ul>
</li>
<li>Wednesday 13th 9:00 - 12:00: Building a Neural Language Model with Blocks by John Arevalo.

<ul>
<li>Notebook: <a href="https://github.com/inulinux12/DL-NLP-workshop/blob/gh-pages/Building_A_Neural_Language_Model_With_Blocks/rnn-tutorial.ipynb">RNN tutorial</a> with blocks.</li>
</ul>
</li>
<li>Wednesday 13th 14:00 - 17:00: Keras and Paraphrasing by Sebastian Sierra.

<ul>
<li>Notebook: <a href="https://github.com/inulinux12/DL-NLP-workshop/blob/gh-pages/Explore_Other_Tools_For_NLM/Keras_NLP.ipynb">Keras and NLP</a>.</li>
<li>Slides: Yet to come.</li>
</ul>
</li>
<li>Thursday 14th 9:00 - 12:00: Tensorflow introduction by Sebastian Otálora.

<ul>
<li>Slides:</li>
<li>Notebook:</li>
</ul>
</li>
<li>Thursday 14th 14:00 - 17:00: Practice: Build a text classification model.

<ul>
<li>Slides:</li>
<li>Notebook:</li>
</ul>
</li>
</ul>

<h2>
<a id="useful-links-and-resources" class="anchor" href="#useful-links-and-resources" aria-hidden="true"><span class="octicon octicon-link"></span></a>Useful links and resources.</h2>

<h3>
<a id="neural-networks-tools" class="anchor" href="#neural-networks-tools" aria-hidden="true"><span class="octicon octicon-link"></span></a>Neural networks tools.</h3>

<p>This is a curated list of neural networks tools and frameworks. Most of them are written in Python and built on top of Theano. Most of them will be available from hal and lisi4 server.    </p>

<ul>
<li>
<a href="http://deeplearning.net/software/theano/">Theano</a>: A numerical computation library for Python.</li>
<li>
<a href="https://github.com/mila-udem/blocks">Blocks</a>: Framework that helps you build neural network models on top of Theano.</li>
<li>
<a href="http://torch.ch/">Torch</a>: Scientific computing framework with wide support for machine learning algorithms using Lua. </li>
<li>
<a href="http://keras.io/">Keras</a>: A minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow or Theano.</li>
<li>
<a href="https://www.tensorflow.org/">TensorFlow</a>: An open source software library for numerical computation using data flow graphs.</li>
<li>Gensim's <a href="https://radimrehurek.com/gensim/models/word2vec.html">word2vec</a>: An useful library built in Python that helps you build and train word2vec models.</li>
</ul>

<h3>
<a id="surveys" class="anchor" href="#surveys" aria-hidden="true"><span class="octicon octicon-link"></span></a>Surveys.</h3>

<p>Surveys and empirical studies are useful resources describing how Recurrent Neural Networks behave. During 2015 there were several works related to this topic and these are some of the most important.    </p>

<ul>
<li>Greff, K., Kumar Srivastava, R., Koutník, J., Steunebrink, B. R., &amp; Schmidhuber, J. (2015). <a href="http://arxiv.org/abs/1503.04069">LSTM: A Search Space Odyssey</a>.</li>
<li>Karpathy, A., Johnson, J., &amp; Fei-Fei, L. (2015). <a href="http://arxiv.org/abs/1506.02078">Visualizing and Understanding Recurrent Networks</a>.</li>
<li>Zachary C. Lipton. (2015). <a href="http://arxiv.org/abs/1506.00019v2">A Critical Review of Recurrent Neural Networks for Sequence Learning</a>.</li>
<li>Jozefowicz, R., Zaremba, W., &amp; Sutskever, I. (2015). <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2015_jozefowicz15.pdf">An Empirical Exploration of Recurrent Network Architectures</a>. In Proceedings of the 32nd International Conference on Machine Learning (Vol. 37).</li>
</ul>

<h3>
<a id="natural-language-processing" class="anchor" href="#natural-language-processing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Natural Language Processing.</h3>

<ul>
<li>
<a href="http://cs224d.stanford.edu/index.html">CS224d</a> Stanford Deep Learning for Natural Language Processing: Course materials <a href="http://cs224d.stanford.edu/syllabus.html">available</a>.</li>
<li>Advances of Deep Learning in NLP by Wojciech Zaremba, New York University and Facebook AI Research: <a href="http://www.cs.nyu.edu/%7Ezaremba/docs/Advances%20in%20deep%20learning%20for%20NLP.pdf">Presentation</a> available.</li>
</ul>

<h3>
<a id="recurrent-neural-networks" class="anchor" href="#recurrent-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recurrent neural networks.</h3>

<p>Although these resources could be in the NLP section, these are created as blogs or pages showing the power of Recurrent neural networks.    </p>

<ul>
<li>
<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> by Andrej Karpathy.</li>
<li>
<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs</a> by Christopher Colah.</li>
<li>
<a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">Recurrent Neural Networks tutorial</a> by <a href="http://www.wildml.com/">wild-ml</a> blog.</li>
</ul>

<h3>
<a id="applications" class="anchor" href="#applications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Applications</h3>

<p>There are several applications of RNN for Natural Language Processing. Language Modelling, Text classification and Machine translation are some of the them.</p>

<h4>
<a id="language-modelling" class="anchor" href="#language-modelling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Language modelling</h4>

<ul>
<li>Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., &amp; Fidler, S. (2015). <a href="https://github.com/ryankiros/skip-thoughts">Skip-Thought Vectors</a>. arXiv Preprint arXiv:1506.06726.</li>
<li>Li, J., Luong, M., &amp; Jurafsky, D. (2015). <a href="http://arxiv.org/pdf/1506.01057">A Hierarchical Neural Autoencoder for Paragraphs and Documents</a>. In Acl 2015.</li>
<li>Tai, K. S., Socher, R., &amp; Manning, C. D. (2015). <a href="http://arxiv.org/abs/1503.00075">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</a>. arXiv Preprint arXiv:1503.00075.</li>
</ul>

<h2>
<a id="further-resources" class="anchor" href="#further-resources" aria-hidden="true"><span class="octicon octicon-link"></span></a>Further resources.</h2>

<p>This list of links is based on Jiwon kim's <a href="http://jiwonkim.org/awesome-rnn">Awesome RNN</a> list of resources.</p>

<h2>
<a id="about-us" class="anchor" href="#about-us" aria-hidden="true"><span class="octicon octicon-link"></span></a>About us</h2>

<p><img src="http://i.imgur.com/CLrXU3M.png" alt="MindLab Research Group"> <a href="https://sites.google.com/a/unal.edu.co/mindlab/">MindLab Research Group</a></p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Deep Learning and Natural Language Processing - workshop maintained by <a href="https://github.com/inulinux12">inulinux12</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
