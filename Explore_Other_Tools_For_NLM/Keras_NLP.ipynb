{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras and Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sebastian Sierra - DL-NLP workshop\n",
    "<img src=\"http://m.memegen.com/4j0k0i.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "* What is Keras?\n",
    "  * Installing Keras\n",
    "* Models in Keras: Sequential vs Graph\n",
    "* Recurrent layers in keras.\n",
    "  * LSTM and bidirectional LSTM\n",
    "  * GRU\n",
    "* Creating new layers in Keras\n",
    "* Ex: Using Keras and gensim to solve semantic similarity task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Keras?\n",
    "\n",
    "It's a Deep Learning library for Theano and TensorFlow. Keras is also built upon four guiding principles:\n",
    "* Modularity.\n",
    "  *  Neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules.\n",
    "* Minimalism.\n",
    "* Easy extensibility.\n",
    "* Work with Python.\n",
    "\n",
    "Keras is suited for easy and fast prototyping. It also supports **convolutional neural networks** and **recurrent neural networks** and easy combination between both. Besides it enables multi-input and multi-output training. Keras runs on GPU or CPU.\n",
    "\n",
    "**Further documentation** can be found on [Keras Docs](http://keras.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras requirements are:\n",
    "* numpy, scipy\n",
    "* pyyaml\n",
    "* HDF5 and h5py\n",
    "* In case of using CNNs: cuDNN\n",
    "\n",
    "In this case we are going to work with **Theano** as backend, so the latest version of **Theano** should be used\n",
    "```bash\n",
    "sudo pip install git+git://github.com/Theano/Theano.git\n",
    "```\n",
    "Finally pip install the latest version of keras\n",
    "```bash\n",
    "sudo pip install keras\n",
    "```\n",
    "Then we check if we have the latest version(>0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "pkg_resources.get_distribution(\"keras\").version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models in Keras: Sequential vs Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are the main structure in Keras. There are two kinds of models: Sequential model and Graph model. Sequential is a sequence of layers, organized in the exact order they where added. Graph models are determined by the connections nodes and the connections between their nodes.\n",
    "\n",
    "Sequential models can be easily created:\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "```\n",
    "Then we can add each layer, in this short example we are creating a network with a Embedding layer as input layer, then we add a LSTM, a Dropout layer, a Dense layer that is a standard fully connected layer and finally an Activation layer using a sigmoid function.\n",
    "```python\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "model.add(Embedding(input_dim, output_dim, input_length=maxlen))\n",
    "model.add(LSTM(output_dim))\n",
    "model.add(Dropout(prob))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "```\n",
    "\n",
    "One example of Keras' easy extensibility is that we can define functions :\n",
    "```python\n",
    "def tanh(x):\n",
    "    return theano.tensor.tanh(x)\n",
    "\n",
    "model.add(Dense(64, activation=tanh))\n",
    "model.add(Activation(tanh))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other side we have Graph models, that can be created so:\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "model = Graph()\n",
    "```\n",
    "In this case we are defining a bidirectional LSTM for a classification problem. Note that in this case we have to define first the input. *maxlen* stands for the input size that our network will have. The details of the construction of a bidirectional LSTM will be further discussed. At the end of the specification of the network we can see that it is really similar to the specification of the previous network.\n",
    "```python\n",
    "model.add_input(name='input', input_shape=(maxlen,), dtype=int)\n",
    "model.add_node(Embedding(input_dim, output_dim, input_length=maxlen),\n",
    "               name='embedding', input='input')\n",
    "model.add_node(LSTM(output_dim), name='forward', input='embedding')\n",
    "model.add_node(LSTM(output_dim, go_backwards=True), name='backward', input='embedding')\n",
    "model.add_node(Dropout(prob), name='dropout', inputs=['forward', 'backward'])\n",
    "model.add_node(Dense(1, activation='sigmoid'), name='sigmoid', input='dropout')\n",
    "model.add_output(name='output', input='sigmoid')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Layers are implemented in Keras. It supports LSTM, GRU and SimpleRNN recurrent layers. Each of one can be called easiy using this:\n",
    "```python\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "```\n",
    "Its input is a 3D tensor with shape **(nb_samples, timesteps, input_dim)**. The output will be 3D tensor with shape  **(nb_samples, timesteps, output_dim)**.\n",
    "\n",
    "Keras by default resets the memory of the recurrent network. In some cases we would like to enable statefulness, so the input of the following iteration is fed with the previous state of the network. This can be done specifying `stateful=True` in the layer constructor.\n",
    "\n",
    "We are going to see how a RNN can be used in text classification task and compare the performance of three basic structures: LSTM, GRU and Bidirectional LSTM. Although we have to set our data ready to use in Keras. Keras has a module with some standard datasets, in our case we will work with the sentiment analysis task of the IMDB reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB sentiment analysis task.\n",
    "Sentiment Analysis is a widely known text classification task. In 2011 was released a dataset composed of 25,000 reviews of movies for training and 25,000 reviews for testing [More info](http://ai.stanford.edu/~amaas/data/sentiment/). As its authors claim, the reviews are highly polar. The labels used for this dataset were 0(Negative review) and 1(Positive review). A negative review has a score <= 4 out of 10, and a positive review has a score >= 7 out of 10. Besides, the training and testing sets contain a disjoint set of movies. We will try to predict if a review contains a positive review or a negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import accuracy\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.datasets import imdb\n",
    "from six.moves import cPickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from utils.helper_keras import sentence_to_wordlist, review_to_words, load_imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define the number of top most frequent words to consider of our Embedding layer, this number will be *max_features*, then we define the maximum length of the input sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we easily load the IMDB data, defining the percentage for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this data is not that interpretable. We are going to upload manually the dataset to see how keras is loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acl_path = \"/data1/aclImdb/\"\n",
    "processed_path = \"/data1/IMDB/\"\n",
    "train = pd.read_csv(processed_path+\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(processed_path+\"labeledTestSet.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how a review looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train[\"review\"][1])\n",
    "print(\"Sentiment: %d\" % (train[\"sentiment\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train[\"review\"][10])\n",
    "print(\"Sentiment: %d\" % (train[\"sentiment\"][10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a list of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train_reviews = [sentence_to_wordlist(review.decode(\"utf8\")) for review in train[\"review\"][:]]\n",
    "corpus_reviews = [sentence_to_wordlist(review.decode(\"utf8\"), tokenized=False) for review in train[\"review\"][:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_reviews[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we calculate the frequency distribution of the terms in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_reviews=' '.join(corpus_reviews)\n",
    "tokens = nltk.word_tokenize(whole_reviews)\n",
    "fdist=FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_df = pd.DataFrame(fdist.items(), columns=['Term', 'Frequency'])\n",
    "ordered_freqdf = freq_df.sort([\"Frequency\"], ascending=[False])\n",
    "ordered_freqdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed_dict = {key: value for (key, value) in zip(ordered_freqdf[\"Term\"][:], range(len(ordered_freqdf[\"Term\"][:])))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "i = 1\n",
    "for review in clean_train_reviews:\n",
    "    tmp = []\n",
    "    for x in review:\n",
    "        if indexed_dict.has_key(x):\n",
    "            tmp.append(indexed_dict[x])\n",
    "    X.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can get again *X_train*, *y_train* and *X_test*, *y_test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_imdb(X, train[\"sentiment\"].tolist(), nb_words=max_features, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the sequences will be padded(where the length is less than 100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the model as we have previously done.  \n",
    "## LSTM\n",
    "<img src=\"https://github.com/Element-Research/rnn/blob/master/doc/image/LSTM.png?raw=true\" style=\"width: 50%; height: 50%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With compile function we can set the objective function, the optimizer and the class evaluation mode. The following objectives are available:\n",
    "* mean_squared_error / mse\n",
    "* root_mean_squared_error / rmse\n",
    "* mean_absolute_error / mae\n",
    "* mean_absolute_percentage_error / mape\n",
    "* mean_squared_logarithmic_error / msle\n",
    "* squared_hinge\n",
    "* hinge\n",
    "* binary_crossentropy: logloss.\n",
    "* categorical_crossentropy: multiclass logloss\n",
    "\n",
    "On the side of the optimizers, Keras provide us these:\n",
    "* SGD\n",
    "* RMSprop\n",
    "* Adagrad\n",
    "* Adadelta\n",
    "* Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can use *fit* function(In a sci-kit learn fashion) to train the model. *evaluate* will show the performance of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=4, validation_data=(X_test, y_test), show_accuracy=True)\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size, show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily use a GRU instead of a LSTM. Most of the code will be similar to the previous one.\n",
    "## GRU\n",
    "<img src=\"https://camo.githubusercontent.com/3ea758e7796a3e21d6b002f7aa588361d7e0bb7b/687474703a2f2f64336b62707a626d63796e6e6d782e636c6f756466726f6e742e6e65742f77702d636f6e74656e742f75706c6f6164732f323031352f31302f53637265656e2d53686f742d323031352d31302d32332d61742d31302e33362e35312d414d2e706e67\" style=\"width: 75%; height: 75%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(GRU(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', class_mode=\"binary\")\n",
    "\n",
    "print(\"----------------------\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=4, validation_data=(X_test, y_test), show_accuracy=True)\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size, show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following case we will use a little more complicated structure. A bidirectional LSTM, it will be built using a Graph model. The main key is to declare two LSTM, one of them have to be enabled to go backward. Unfortunately documentation about this functionality is not clear.  \n",
    "<img src=\"http://zhaoshuaijiang.com/paper_image/Bidirectional_RNN.png\" />\n",
    "M. Schuster and K. K. Paliwal. [Bidirectional Recurrent Neural Networks](http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf). IEEE Transactions on Signal\n",
    "Processing, vol. 45, pp. 2673–2681, 1997."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Graph()\n",
    "model.add_input(name='input', input_shape=(maxlen,), dtype=int)\n",
    "model.add_node(Embedding(max_features, 128, input_length=maxlen),\n",
    "               name='embedding', input='input')\n",
    "model.add_node(LSTM(64), name='forward', input='embedding')\n",
    "model.add_node(LSTM(64, go_backwards=True), name='backward', input='embedding')\n",
    "model.add_node(Dropout(0.5), name='dropout', inputs=['forward', 'backward'])\n",
    "model.add_node(Dense(1, activation='sigmoid'), name='sigmoid', input='dropout')\n",
    "model.add_output(name='output', input='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time instead of using *evaluate* function, we will evaluate it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile('adam', {'output': 'binary_crossentropy'})\n",
    "\n",
    "print('--------------------')\n",
    "model.fit({'input': X_train, 'output': y_train}, batch_size=batch_size, nb_epoch=4)\n",
    "acc = accuracy(y_test, np.round(np.array(model.predict({'input': X_test},\n",
    "                                               batch_size=batch_size)['output'])))\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sense of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to perform prediction using this model. Let's take a negative and a positive example. (From the test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_test_reviews = [sentence_to_wordlist(review.decode(\"utf8\")) for review in test[\"review\"][:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(' '.join(clean_test_reviews[2]))\n",
    "print(\"Sentiment= %d\" % (test[\"sentiment\"][2]))\n",
    "print(' '.join(clean_test_reviews[4]))\n",
    "print(\"Sentiment= %d\" % (test[\"sentiment\"][4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a neutral review to see how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_X = []\n",
    "i = 1\n",
    "additional_examples = \"This movie was amazing, though, I did't like when Tyrion dies.\"\n",
    "\n",
    "for review in [clean_test_reviews[2], clean_test_reviews[4], sentence_to_wordlist(additional_examples)]:\n",
    "    tmp = []\n",
    "    for x in review:\n",
    "        if indexed_dict.has_key(x):\n",
    "            tmp.append(indexed_dict[x])\n",
    "    testing_X.append(tmp)\n",
    "(new_X, new_y), _ = load_imdb(testing_X, [1, 0, 1], nb_words=max_features, test_split=0.)\n",
    "new_X = sequence.pad_sequences(new_X, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a problem where you'd want to predict chains larger than 100 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(new_X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict({'input': np.array(new_X)}, batch_size=batch_size)['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras and gensim to solve Semantic Similarity task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: Now we are going to apply Keras in another NLP task. Semantic Similarity task has become a central problem in NLP. Recently a dataset was introduced for the Semeval, it was named SICK((Sentences Involving Compositional Knowledge). Further info can be found at [SICK dataset](http://clic.cimec.unitn.it/composes/sick.html). The SICK data set consists of about 10,000 English sentence pairs, generated starting from two existing sets: the 8K ImageFlickr data set and the SemEval 2012 STS MSR-Video Description data set. Each sentence pair was annotated for relatedness by means of crowdsourcing techniques. In the final set, gold scores were distributed as follows: the relatednes scoring resulted in 923 pairs within the [1,2) range, 1373 pairs within the [2,3) range, 3872 pairs within the [3,4) range, and 3672 pairs within the [4,5] range. This exercise is part of Skip-thoughts work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we will define the architecture to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy       \n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sick_data=\"/home/datasets/datasets1/skip_thoughts_models/skip-thoughts/data/\"\n",
    "model_path = \"/home/datasets/datasets1/word2vec-embeddings/GoogleNews-vectors-negative300.bin.gz\"\n",
    "def prepare_model(ninputs=9600, nclass=5):\n",
    "    \"\"\"\n",
    "    Set up and compile the model architecture (Logistic regression)\n",
    "    \"\"\"\n",
    "    lrmodel = Sequential()\n",
    "    lrmodel.add(Dense(ninputs))\n",
    "    lrmodel.add(Activation('softmax'))\n",
    "    lrmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return lrmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(lrmodel, X, Y, devX, devY, devscores):\n",
    "    \"\"\"\n",
    "    Train model, using pearsonr on dev for early stopping\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    best = -1.0\n",
    "    r = np.arange(1,6)\n",
    "\n",
    "    while not done:\n",
    "        # Every 100 epochs, check Pearson on development set\n",
    "        lrmodel.fit(X, Y, verbose=2, shuffle=False, validation_data=(devX, devY))\n",
    "        yhat = np.dot(lrmodel.predict_proba(devX, verbose=2), r)\n",
    "        score = pearsonr(yhat, devscores)[0]\n",
    "        if score > best:\n",
    "            print(score)\n",
    "            best = score\n",
    "            bestlrmodel = copy.deepcopy(lrmodel)\n",
    "        else:\n",
    "            done = True\n",
    "\n",
    "    yhat = np.dot(bestlrmodel.predict_proba(devX, verbose=2), r)\n",
    "    score = pearsonr(yhat, devscores)[0]\n",
    "    print('Dev Pearson: ' + str(score))\n",
    "    return bestlrmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_labels(labels, nclass=5):\n",
    "    \"\"\"\n",
    "    Label encoding from Tree LSTM paper (Tai, Socher, Manning)\n",
    "    \"\"\"\n",
    "    Y = np.zeros((len(labels), nclass)).astype('float32')\n",
    "    for j, y in enumerate(labels):\n",
    "        for i in range(nclass):\n",
    "            if i+1 == np.floor(y) + 1:\n",
    "                Y[j,i] = y - np.floor(y)\n",
    "            if i+1 == np.floor(y):\n",
    "                Y[j,i] = np.floor(y) - y + 1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(loc=sick_data):                                                                                                      \n",
    "    \"\"\"                                                                                                                            \n",
    "    Load SICK\n",
    "    \"\"\"                                                                                                                            \n",
    "    trainA, trainB, devA, devB, testA, testB = [],[],[],[],[],[]                                                                   \n",
    "    trainS, devS, testS = [],[],[]                                                                                                 \n",
    "                                                                                                                                   \n",
    "    with open(loc + 'SICK_train.txt', 'rb') as f:                                                                                  \n",
    "        for line in f:                                                                                                             \n",
    "            text = line.strip().split('\\t')                                                                                        \n",
    "            trainA.append(text[1])                                                                                                 \n",
    "            trainB.append(text[2])                                                                                                 \n",
    "            trainS.append(text[3])                                                                                                 \n",
    "    with open(loc + 'SICK_trial.txt', 'rb') as f:                                                                                  \n",
    "        for line in f:                                                                                                             \n",
    "            text = line.strip().split('\\t')                                                                                        \n",
    "            devA.append(text[1])                                                                                                   \n",
    "            devB.append(text[2])                                                                                                   \n",
    "            devS.append(text[3])                                                                                                   \n",
    "    with open(loc + 'SICK_test_annotated.txt', 'rb') as f:                                                                         \n",
    "        for line in f:                                                                                                             \n",
    "            text = line.strip().split('\\t')                                                                                        \n",
    "            testA.append(text[1])                                                                                                  \n",
    "            testB.append(text[2])                                                                                                  \n",
    "            testS.append(text[3])                                                                                                  \n",
    "                                                                                                                                   \n",
    "    trainS = [float(s) for s in trainS[1:]]                                                                                        \n",
    "    devS = [float(s) for s in devS[1:]]                                                                                            \n",
    "    testS = [float(s) for s in testS[1:]]                                                                                          \n",
    "                                                                                                                                   \n",
    "    return [trainA[1:], trainB[1:]], [devA[1:], devB[1:]], [testA[1:], testB[1:]], [trainS, devS, testS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_word2vec(model, dataset):\n",
    "    #model = Word2Vec.load_word2vec_format(model_path, binary=True)\n",
    "    #Replace anything but a character for a space, lowercase everything and tokenize\n",
    "    #Pending to add stop words\n",
    "    trainA = [word_tokenize(re.sub(\"[^a-zA-Z]\", \" \", t).lower()) for t in dataset[0][:]]\n",
    "    trainB = [word_tokenize(re.sub(\"[^a-zA-Z]\", \" \", t).lower()) for t in dataset[1][:]]\n",
    "    feat_trainA = [[model[t] for t in sentence if model.vocab.has_key(t) ] for sentence in trainA]\n",
    "    feat_trainB = [[model[t] for t in sentence if model.vocab.has_key(t) ] for sentence in trainB]\n",
    "    return feat_trainA, feat_trainB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, dev, test, scores = load_data()                                                                                         \n",
    "train[0], train[1], scores[0] = shuffle(train[0], train[1], scores[0], random_state=1234)\n",
    "model = Word2Vec.load_word2vec_format(model_path, binary=True)\n",
    "feat_trainA, feat_trainB = encode_word2vec(model, train)\n",
    "feat_devA, feat_devB = encode_word2vec(model, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg_featA = np.array([np.sum(sentence, axis=0) for sentence in feat_trainA])\n",
    "agg_featB = np.array([np.sum(sentence, axis=0) for sentence in feat_trainB])\n",
    "agg_devA = np.array([np.sum(sentence, axis=0) for sentence in feat_devA])\n",
    "agg_devB = np.array([np.sum(sentence, axis=0) for sentence in feat_devB])\n",
    "trainF = np.c_[np.abs(agg_featA - agg_featB), agg_featA * agg_featB]\n",
    "devF = np.c_[np.abs(agg_devA - agg_devB), agg_devA * agg_devB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainY = encode_labels(scores[0])\n",
    "devY = encode_labels(scores[1])\n",
    "lrmodel = prepare_model(ninputs=trainF.shape[1])\n",
    "bestlrmodel = train_model(lrmodel, trainF, trainY, devF, devY, scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_testA, feat_testB = encode_word2vec(model, test)\n",
    "agg_testA = np.array([np.sum(sentence, axis=0) for sentence in feat_testA])\n",
    "agg_testB = np.array([np.sum(sentence, axis=0) for sentence in feat_testB])\n",
    "testF = np.c_[np.abs(agg_testA - agg_testB), agg_testA * agg_testB]\n",
    "\n",
    "print 'Evaluating...'\n",
    "r = np.arange(1,6)\n",
    "yhat = np.dot(bestlrmodel.predict_proba(testF, verbose=2), r)\n",
    "pr = pearsonr(yhat, scores[2])[0]\n",
    "sr = spearmanr(yhat, scores[2])[0]\n",
    "se = mse(yhat, scores[2])\n",
    "print 'Test Pearson: ' + str(pr)\n",
    "print 'Test Spearman: ' + str(sr)\n",
    "print 'Test MSE: ' + str(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About us\n",
    "<img src=\"https://sites.google.com/a/unal.edu.co/mindlab/_/rsrc/1353286903227/config/customLogo.gif?revision=10\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
